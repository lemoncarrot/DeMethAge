# -*- coding: utf-8 -*-
"""SEFH 2024 NN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GNzsrZEivafAyUmUtxpk0d0K6W1Lsgmg
"""

import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
import numpy as np
from keras.optimizers import Adam, SGD, RMSprop
from sklearn.linear_model import ElasticNet
from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer, r2_score
from sklearn.model_selection import GridSearchCV



df = pd.read_csv("hannum.csv", index_col=0)

#df = pd.concat([df1, df2], ignore_index=True)

#df2 = pd.read_csv("GSE61496_nn.csv", index_col=0)
#df3 = pd.read_csv("GSE174555_nn.csv", row_names=1)
#df4 = pd.read_csv("GSE121633_nn.csv", index_col=0)

"""
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

def sin_activation(x):
    return 0.5 * (tf.math.sin(x) + 1)

x = np.linspace(-10, 10, 100)

y = sin_activation(x)

plt.figure(figsize=(6.5, 4))
plt.plot(x, y, label='Sin')
plt.title('Sin Activation Function')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid(True)
plt.legend()
plt.show()
"""



#import pandas as pd
#df2 = pd.read_csv("GSE61496_nn.csv", index_col=0)

#df2 = pd.read_csv("GSE61496_nn.csv", index_col=0)
#df3 = pd.read_csv("GSE174555_nn.csv", row_names=1)
#df4 = pd.read_csv("GSE121633_nn.csv", index_col=0)

#df = pd.concat([df1, df2], ignore_index=True)

#print(df.head)



x = np.transpose(df.iloc[1:-12, :])
#x = np.transpose(df.iloc[1:-12, :])
y = df.iloc[0, :]

cell_counts = df.iloc[-12:, :].transpose()
cell_counts=(cell_counts-cell_counts.mean())/cell_counts.std()

print(cell_counts.head)

print(x.shape)
print(y.shape)
print(y)
print(x.tail(12))



X_train, X_temp, Y_train, Y_temp = train_test_split(x, y, test_size=0.3, random_state=27)
X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=27)

def sin_activation(x):
    return 0.5 * (tf.math.sin(x) + 1)

#x_values = np.linspace(-2*np.pi, 2*np.pi, 400)
#y_values = sin_activation(x_values)
#
#import matplotlib.pyplot as plt
#plt.plot(x_values, y_values)
#plt.grid(True)
#plt.show()

#model = keras.Sequential([
#    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),
#    Dense(128, activation=sin_activation),
 #   Dense(128, activation='relu'),
#    Dense(64, activation = 'relu'),
#    Dense(32, activation = 'relu'),
#    Dense(1)
#])

from tensorflow.keras.optimizers import Adam
custom_learning_rate = 0.01
adam_optimizer = Adam(learning_rate=custom_learning_rate)

model = keras.Sequential([
    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(128, activation=sin_activation),
    Dense(128, activation='relu'),
    Dense(64, activation = 'relu'),
    Dense(32, activation = 'relu'),
    Dense(1)
])

model.compile(
    loss='mean_squared_error',
    optimizer=adam_optimizer,
    metrics=['mean_absolute_error']
)

#history = model.fit(
#    X_train, Y_train,
#   epochs=66,
#    batch_size=128,
#    validation_data=(X_val, Y_val)
#) #R-squared: 0.7555681061607531
history = model.fit(
    X_train, Y_train,
    epochs=66,
    validation_data=(X_val, Y_val)
) #R-squared: 0.8775625654364946


test_loss, test_mae = model.evaluate(X_test, Y_test)

predicted_age = model.predict(X_test)
new_age = pd.DataFrame(predicted_age, columns=['Predicted_Age'])

final_df = pd.DataFrame(predicted_age, columns=['Predicted_Age'], index=X_test.index)
final_df = pd.concat([final_df, cell_counts], axis=1)
final_df = final_df.dropna(subset=['Predicted_Age'])

print(final_df.head)
na_count = final_df['Predicted_Age'].isna().sum()
print(f"Number of NA values in 'Predicted_Age': {na_count}")

#print(new_age)
#print(new_age.size)
#print(Y_test.size)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

new_age = np.ravel(new_age)
Y_test = np.ravel(Y_test)

correlation_matrix = np.corrcoef(new_age, Y_test)
correlation_xy = correlation_matrix[0,1]
r_squared = correlation_xy**2

print("R-squared:", r_squared)

Y_test = Y_test.flatten()
predicted_age = predicted_age.flatten()

plt.figure(figsize=(8, 6))
sns.scatterplot(x=Y_test, y=predicted_age)
plt.plot(Y_test, np.poly1d(np.polyfit(Y_test, predicted_age, 1))(Y_test), color="red")  # Regression line
plt.xlabel('Actual Age')
plt.ylabel('Predicted Age')
plt.title('Scatter Plot with Regression Line: Predicted Age vs Actual Age')
plt.show()

#regression model predicting adjusted age with predicted age, predictedage^2, and cell counts

Y = Y_test
X = final_df
from sklearn.linear_model import ElasticNet
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, r2_score


r2_scorer = make_scorer(r2_score)
param_grid = {
    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0],
    'l1_ratio': [0.0, 0.25, 0.5, 0.75, 1.0]
}

grid_search = GridSearchCV(
    ElasticNet(random_state=42),
    param_grid,
    cv=10,
    scoring=r2_scorer,
    verbose=1
)

grid_search.fit(X, Y)
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(f"Best Parameters: {best_params}")
print(f"Best R-squared Score: {best_score}")

print(final_df)

elastic_net_model = ElasticNet(alpha=0.1, l1_ratio=0.0, random_state=42)

elastic_net_model.fit(final_df, Y_test)

predicted_values = elastic_net_model.predict(final_df)

plt.scatter(Y_test, predicted_values)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values')
plt.plot([min(Y_test), max(Y_test)], [min(Y_test), max(Y_test)], color='red')  # Diagonal line
plt.show()

r_squared = r2_score(Y_test, predicted_values)
print(f"R-squared: {r_squared}")

plt.scatter(final_df.iloc[:, 12], (predicted_values/Y_test))
plt.xlabel('Cell Type Bas')
plt.ylabel('Difference')
#plt.plot([min(Y_test), max(Y_test)], [min(Y_test), max(Y_test)], color='red')  # Diagonal line
plt.show()



best_val_score = float('inf')

def create_model(optimizer, learning_rate):
    model = keras.Sequential([
        Dense(256, activation='relu', input_shape=(X_train.shape[1],)),
        Dense(128, activation=sin_activation),
        Dense(64, activation='relu'),
        Dense(32, activation='sigmoid'),
        Dense(1)
    ])
    optimizer_instance = optimizer(learning_rate=learning_rate)
    model.compile(optimizer=optimizer_instance, loss='mse')  # Use Mean Squared Error for regression
    return model

optimizers = [Adam, SGD, RMSprop]
learning_rates = [0.01, 0.001, 0.0001]

for optimizer in optimizers:
    for lr in learning_rates:
        model = create_model(optimizer, lr)
        history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=50, verbose=1)
        val_score = min(history.history['val_loss'])

        if val_score < best_val_score:
            best_val_score = val_score
            best_optimizer = optimizer
            best_learning_rate = lr

print(f"Best Optimizer: {best_optimizer.__name__}, Best Learning Rate: {best_learning_rate}")
#adam, 0.001

from keras.callbacks import EarlyStopping

model = keras.Sequential([
    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(128, activation=sin_activation),
    Dense(128, activation='relu'),
    Dense(64, activation = 'relu'),
    Dense(1)
])

model.compile(optimizer="adam", loss='mse')

early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)

history = model.fit(
    X_train, Y_train,
    validation_data=(X_val, Y_val),
    epochs=1000,
    verbose=1,
    callbacks=[early_stopping]
)

optimal_epochs = early_stopping.stopped_epoch - early_stopping.patience
print(f"The optimal number of epochs is: {optimal_epochs}")
#66 epochs

best_optimizer = 'adam'
best_learning_rate = 0.001

def create_model(neurons_layer_1, neurons_layer_2, neurons_layer_3, neurons_layer_4):
    model = keras.Sequential([
        Dense(neurons_layer_1, activation='relu', input_shape=(X_train.shape[1],)),
        Dense(neurons_layer_2, activation=sin_activation),
        Dense(neurons_layer_3, activation='relu'),
        Dense(neurons_layer_4, activation='relu'),
        Dense(1)  # Output layer for regression
    ])
    model.compile(optimizer=best_optimizer, loss='mse')
    return model

best_val_score = float('inf')
best_architecture = (0, 0, 0, 0)

neurons_options = [32, 64, 128, 256, 512]

for neurons_1 in neurons_options:
    for neurons_2 in neurons_options:
        for neurons_3 in neurons_options:
          for neurons_4 in neurons_options:
            model = create_model(neurons_1, neurons_2, neurons_3, neurons_4)
            history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, verbose=1)
            val_score = model.evaluate(X_val, Y_val, verbose=1)
            if val_score < best_val_score:
                best_val_score = val_score
                best_architecture = (neurons_1, neurons_2, neurons_3)

print(f"Best architecture: {best_architecture} with validation score: {best_val_score}")
#not enough ram
